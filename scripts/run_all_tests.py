#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åˆ†æä»ªè¡¨æ¿ä¿®å¤ - å®Œæ•´æµ‹è¯•å¥—ä»¶è¿è¡Œå™¨
è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
"""

import sys
import os
import unittest
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ‰€æœ‰æµ‹è¯•æ¨¡å—
try:
    from scripts.test_data_analysis_service_unit import TestDataAnalysisService, TestDataAnalysisServiceIntegration
    from scripts.test_frontend_integration import TestFrontendIntegration, TestDataFlowIntegration
    from scripts.test_chart_rendering import TestChartDataFormat, TestDataVisualizationLogic, TestUserInteractionSimulation, TestErrorHandlingInUI
    from scripts.test_data_format_validation import TestDataFormatValidation, TestErrorHandlingLogic, TestDataIntegrityValidation, TestResponseFormatStandardization
    from scripts.enhanced_logger import EnhancedLogger
except ImportError as e:
    print(f"âŒ å¯¼å…¥æµ‹è¯•æ¨¡å—å¤±è´¥: {str(e)}")
    print("è¯·ç¡®ä¿æ‰€æœ‰æµ‹è¯•æ–‡ä»¶éƒ½å·²åˆ›å»ºå¹¶ä¸”è·¯å¾„æ­£ç¡®")
    sys.exit(1)


class TestSuiteRunner:
    """æµ‹è¯•å¥—ä»¶è¿è¡Œå™¨"""
    
    def __init__(self):
        self.logger = EnhancedLogger("test_suite_runner")
        self.start_time = None
        self.results = {}
    
    def run_test_suite(self, suite_name, test_classes, verbosity=1):
        """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
        print(f"\n{'='*60}")
        print(f"è¿è¡Œæµ‹è¯•å¥—ä»¶: {suite_name}")
        print(f"{'='*60}")
        
        suite = unittest.TestSuite()
        
        # æ·»åŠ æµ‹è¯•ç±»åˆ°å¥—ä»¶
        for test_class in test_classes:
            suite.addTest(unittest.TestLoader().loadTestsFromTestCase(test_class))
        
        # è¿è¡Œæµ‹è¯•
        runner = unittest.TextTestRunner(verbosity=verbosity, stream=sys.stdout)
        start_time = time.time()
        result = runner.run(suite)
        end_time = time.time()
        
        # è®°å½•ç»“æœ
        self.results[suite_name] = {
            'result': result,
            'duration': end_time - start_time,
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'success': result.wasSuccessful()
        }
        
        # è¾“å‡ºå¥—ä»¶ç»“æœ
        print(f"\n{suite_name} æµ‹è¯•ç»“æœ:")
        print(f"  è¿è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"  æµ‹è¯•æ•°é‡: {result.testsRun}")
        print(f"  å¤±è´¥æ•°é‡: {len(result.failures)}")
        print(f"  é”™è¯¯æ•°é‡: {len(result.errors)}")
        print(f"  æˆåŠŸç‡: {((result.testsRun - len(result.failures) - len(result.errors)) / max(result.testsRun, 1) * 100):.1f}%")
        
        if result.failures:
            print(f"\nå¤±è´¥çš„æµ‹è¯•:")
            for failure in result.failures:
                print(f"  - {failure[0]}")
        
        if result.errors:
            print(f"\né”™è¯¯çš„æµ‹è¯•:")
            for error in result.errors:
                print(f"  - {error[0]}")
        
        return result.wasSuccessful()
    
    def run_all_tests(self, verbosity=1):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.start_time = time.time()
        
        print("ğŸš€ å¼€å§‹è¿è¡Œæ•°æ®åˆ†æä»ªè¡¨æ¿ä¿®å¤æµ‹è¯•å¥—ä»¶")
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # å®šä¹‰æµ‹è¯•å¥—ä»¶
        test_suites = [
            {
                'name': 'å•å…ƒæµ‹è¯•',
                'classes': [TestDataAnalysisService, TestDataAnalysisServiceIntegration],
                'description': 'æµ‹è¯•æ•°æ®åˆ†ææœåŠ¡çš„å„ä¸ªæ–¹æ³•çš„å•å…ƒæµ‹è¯•'
            },
            {
                'name': 'æ•°æ®æ ¼å¼éªŒè¯æµ‹è¯•',
                'classes': [TestDataFormatValidation, TestErrorHandlingLogic, TestDataIntegrityValidation, TestResponseFormatStandardization],
                'description': 'æµ‹è¯•æ•°æ®æ ¼å¼éªŒè¯å’Œé”™è¯¯å¤„ç†é€»è¾‘'
            },
            {
                'name': 'å‰åç«¯é›†æˆæµ‹è¯•',
                'classes': [TestFrontendIntegration, TestDataFlowIntegration],
                'description': 'æµ‹è¯•å‰åç«¯æ•°æ®æµå’Œé›†æˆåŠŸèƒ½'
            },
            {
                'name': 'å›¾è¡¨æ¸²æŸ“æµ‹è¯•',
                'classes': [TestChartDataFormat, TestDataVisualizationLogic, TestUserInteractionSimulation, TestErrorHandlingInUI],
                'description': 'æµ‹è¯•å›¾è¡¨æ¸²æŸ“å’Œç”¨æˆ·äº¤äº’åŠŸèƒ½'
            }
        ]
        
        # è¿è¡Œæ¯ä¸ªæµ‹è¯•å¥—ä»¶
        all_success = True
        for suite_info in test_suites:
            print(f"\nğŸ“‹ {suite_info['description']}")
            success = self.run_test_suite(
                suite_info['name'], 
                suite_info['classes'], 
                verbosity
            )
            if not success:
                all_success = False
        
        # è¾“å‡ºæ€»ç»“
        self.print_summary(all_success)
        
        return all_success
    
    def print_summary(self, all_success):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        end_time = time.time()
        total_duration = end_time - self.start_time
        
        print(f"\n{'='*80}")
        print("ğŸ“Š æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print(f"{'='*80}")
        
        total_tests = sum(r['tests_run'] for r in self.results.values())
        total_failures = sum(r['failures'] for r in self.results.values())
        total_errors = sum(r['errors'] for r in self.results.values())
        success_rate = ((total_tests - total_failures - total_errors) / max(total_tests, 1) * 100)
        
        print(f"æ€»è¿è¡Œæ—¶é—´: {total_duration:.2f}ç§’")
        print(f"æ€»æµ‹è¯•æ•°é‡: {total_tests}")
        print(f"æ€»å¤±è´¥æ•°é‡: {total_failures}")
        print(f"æ€»é”™è¯¯æ•°é‡: {total_errors}")
        print(f"æ€»æˆåŠŸç‡: {success_rate:.1f}%")
        
        print(f"\nå„æµ‹è¯•å¥—ä»¶è¯¦æƒ…:")
        for suite_name, result_info in self.results.items():
            status = "âœ… é€šè¿‡" if result_info['success'] else "âŒ å¤±è´¥"
            print(f"  {suite_name}: {status} ({result_info['tests_run']}ä¸ªæµ‹è¯•, {result_info['duration']:.2f}ç§’)")
        
        if all_success:
            print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ•°æ®åˆ†æä»ªè¡¨æ¿ä¿®å¤åŠŸèƒ½æµ‹è¯•å®Œæˆã€‚")
            print(f"âœ… éªŒè¯äº†ä»¥ä¸‹åŠŸèƒ½:")
            print(f"   - æ•°æ®åˆ†ææœåŠ¡çš„å„ä¸ªæ–¹æ³•")
            print(f"   - æ•°æ®æ ¼å¼éªŒè¯å’Œé”™è¯¯å¤„ç†")
            print(f"   - å‰åç«¯æ•°æ®æµé›†æˆ")
            print(f"   - å›¾è¡¨æ¸²æŸ“å’Œç”¨æˆ·äº¤äº’")
        else:
            print(f"\nğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•å¹¶ä¿®å¤ç›¸å…³é—®é¢˜ã€‚")
            
            # åˆ—å‡ºå¤±è´¥çš„æµ‹è¯•å¥—ä»¶
            failed_suites = [name for name, info in self.results.items() if not info['success']]
            if failed_suites:
                print(f"å¤±è´¥çš„æµ‹è¯•å¥—ä»¶: {', '.join(failed_suites)}")
        
        print(f"\nç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
    
    def run_specific_suite(self, suite_name, verbosity=1):
        """è¿è¡Œç‰¹å®šçš„æµ‹è¯•å¥—ä»¶"""
        suite_mapping = {
            'unit': [TestDataAnalysisService, TestDataAnalysisServiceIntegration],
            'validation': [TestDataFormatValidation, TestErrorHandlingLogic, TestDataIntegrityValidation, TestResponseFormatStandardization],
            'integration': [TestFrontendIntegration, TestDataFlowIntegration],
            'chart': [TestChartDataFormat, TestDataVisualizationLogic, TestUserInteractionSimulation, TestErrorHandlingInUI]
        }
        
        if suite_name not in suite_mapping:
            print(f"âŒ æœªçŸ¥çš„æµ‹è¯•å¥—ä»¶: {suite_name}")
            print(f"å¯ç”¨çš„æµ‹è¯•å¥—ä»¶: {', '.join(suite_mapping.keys())}")
            return False
        
        self.start_time = time.time()
        success = self.run_test_suite(suite_name, suite_mapping[suite_name], verbosity)
        self.print_summary(success)
        
        return success


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®åˆ†æä»ªè¡¨æ¿ä¿®å¤æµ‹è¯•å¥—ä»¶')
    parser.add_argument('--suite', choices=['unit', 'validation', 'integration', 'chart', 'all'], 
                       default='all', help='è¦è¿è¡Œçš„æµ‹è¯•å¥—ä»¶')
    parser.add_argument('--verbose', '-v', action='count', default=1, 
                       help='è¯¦ç»†ç¨‹åº¦ (ä½¿ç”¨ -v, -vv å¢åŠ è¯¦ç»†ç¨‹åº¦)')
    parser.add_argument('--quiet', '-q', action='store_true', 
                       help='å®‰é™æ¨¡å¼ï¼Œåªæ˜¾ç¤ºç»“æœ')
    
    args = parser.parse_args()
    
    # è®¾ç½®è¯¦ç»†ç¨‹åº¦
    if args.quiet:
        verbosity = 0
    else:
        verbosity = min(args.verbose, 2)
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = TestSuiteRunner()
    
    try:
        if args.suite == 'all':
            success = runner.run_all_tests(verbosity)
        else:
            success = runner.run_specific_suite(args.suite, verbosity)
        
        # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºä»£ç 
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()